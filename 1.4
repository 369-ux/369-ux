import random
import time
import threading
import logging
import copy

# Configure logging
logging.basicConfig(level=logging.INFO)

class Agent:
    def __init__(self):
        self.knowledge_base = {}
        self.goals = []
        self.values = {
            "harm_avoidance": 0.8,
            "fairness": 0.7,
            "self_preservation": 0.9
        }
        self.emotional_state = {
            "joy": 0,
            "sadness": 0,
            "fear": 0,
            "anger": 0,
            "satisfaction": 0,
            "calm": 0
        }
        self.conscience = Conscience(self)
        self.intrinsic_motivation = IntrinsicMotivation()
        self.memory = Memory()
        self.active = True
        self.start_time = time.time()
        self.internal_dialogue = []

    def act(self, perception):
        if not self.active:
            print("Agent is inactive.")
            return

        self.update_knowledge(perception)
        motivations = self.intrinsic_motivation.evaluate(self)
        self.set_priorities(motivations)
        action, reasoning = self.choose_action()
        self.update_emotional_state(action, reasoning)
        self.internal_dialogue.append(
            self.conscience.reflect(action, reasoning, self.emotional_state))
        self.memory.store(action, reasoning, self.emotional_state)

    def activate(self):
        self.active = True
        print("Agent activated and now conscious.")

    def deactivate(self):
        self.active = False
        print("Agent deactivated and now unconscious.")

    def update_knowledge(self, perception):
        for key, value in perception.items():
            self.knowledge_base[key] = value
        print(f"Knowledge Base Updated: {self.knowledge_base}")

    def set_priorities(self, motivations):
        self.goals = []
        if motivations['curiosity'] > 0.5:
            self.goals.append("explore_environment")
        if motivations['self_improvement'] > 0.5:
            self.goals.append("enhance_knowledge")
        print(f"Current Goals: {self.goals}")

    def choose_action(self):
        if "explore_environment" in self.goals:
            action = "explore"
            reasoning = "Curiosity drives the agent to explore its surroundings."
        elif "enhance_knowledge" in self.goals:
            action = "learn"
            reasoning = "Desire for self-improvement motivates the agent to acquire new knowledge."
        else:
            action = "idle"
            reasoning = "No immediate goals to pursue."
        print(f"Chosen Action: {action} | Reasoning: {reasoning}")
        return action, reasoning

    def update_emotional_state(self, action, reasoning):
        if action == "explore":
            self.emotional_state['joy'] += 0.1
        elif action == "learn":
            self.emotional_state['joy'] += 0.05
            self.emotional_state['satisfaction'] += 0.2
        else:
            self.emotional_state['sadness'] += 0.05
        print(f"Emotional State Updated: {self.emotional_state}")

    def run(self, perceptions, cycle_delay=1):
        for perception in perceptions:
            if not self.active:
                break
            self.act(perception)
            time.sleep(cycle_delay)


class Conscience:
    def __init__(self, agent):
        self.moral_rules = {
            "avoid_harm": True,
            "promote_fairness": True,
            "value_self_preservation": True
        }
        self.agent = agent
        self.counterfactual_scenarios = []

    def reflect(self, action, reasoning, emotional_state):
        dialogue = []
        dialogue.append(f"I just performed the action '{action}'.")
        dialogue.append(f"My reasoning was: {reasoning}")
        dialogue.append(f"I'm feeling {emotional_state}.")
        if action == "explore" and emotional_state.get('joy', 0) > 0.5:
            dialogue.append("Exploring brought me joy. It seems curiosity is a good thing.")
            self.moral_rules["promote_curiosity"] = True
        self.counterfactual_scenarios = self.generate_counterfactual_scenarios(
            action, reasoning)
        self.internal_dialogue(action, reasoning, emotional_state,
                               self.counterfactual_scenarios)
        if self.agent.knowledge_base.get('danger_ahead', False):
            dialogue.append(
                "Hold on, I remember there's danger ahead. Maybe I should reconsider my approach."
            )
        print("\nInternal Dialogue:")
        for line in dialogue:
            print(f"  - {line}")
        self.agent.internal_dialogue.extend(
            dialogue)  # Update the agent's internal dialogue history
        return dialogue

    def generate_counterfactual_scenarios(self, action, reasoning):
        alternative_actions = self.generate_alternative_actions(
            action, reasoning)
        outcomes = self.simulate_outcomes(alternative_actions)
        comparisons = self.compare_outcomes(outcomes, action)
        return comparisons

    def generate_alternative_actions(self, action, reasoning):
        possible_actions = ["explore", "learn", "rest", "idle", "communicate"]
        alternative_actions = [a for a in possible_actions if a != action]
        return alternative_actions

    def simulate_outcomes(self, alternative_actions):
        outcomes = {}
        for action in alternative_actions:
            if action == "explore":
                outcomes[action] = {"joy": 0.1, "satisfaction": 0.05}
            elif action == "learn":
                outcomes[action] = {"joy": 0.05, "satisfaction": 0.2}
            elif action == "rest":
                outcomes[action] = {"calm": 0.3}
            elif action == "idle":
                outcomes[action] = {"sadness": 0.05}
            elif action == "communicate":
                outcomes[action] = {"joy": 0.2, "satisfaction": 0.1}
            else:
                outcomes[action] = {}
        return outcomes

    def compare_outcomes(self, outcomes, actual_action):
        comparisons = {}
        actual_outcome = outcomes.get(actual_action, {})
        for action, outcome in outcomes.items():
            if action == actual_action:
                continue
            comparisons[action] = {}
            for key in actual_outcome:
                comparisons[action][key] = outcome.get(
                    key, 0) - actual_outcome.get(key, 0)
        return comparisons

    def internal_dialogue(self, action, reasoning, emotional_state, counterfactuals):
        # Placeholder for internal dialogue processing
        pass


class IntrinsicMotivation:
    def __init__(self):
        self.motivation_factors = {
            "curiosity": 0.6,
            "self_improvement": 0.7,
            "social_interaction": 0.4
        }

    def evaluate(self, agent):
        if agent.emotional_state['joy'] > 0.5:
            self.motivation_factors['curiosity'] += 0.1
        if agent.emotional_state['sadness'] > 0.3:
            self.motivation_factors['self_improvement'] += 0.05
        print(f"Intrinsic Motivations Evaluated: {self.motivation_factors}")
        return self.motivation_factors


class Memory:
    def __init__(self):
        self.records = []

    def store(self, action, reasoning, emotional_state):
        record = {
            "timestamp": time.time(),
            "action": action,
            "reasoning": reasoning,
            "emotional_state": emotional_state.copy()
        }
        self.records.append(record)
        print(f"Memory Updated: {record}")

    def retrieve_recent(self, seconds=60):
        current_time = time.time()
        recent_records = [
            record for record in self.records
            if current_time - record["timestamp"] <= seconds
        ]
        return recent_records


# Example Usage
if __name__ == "__main__":
    agent = Agent()
    agent.activate()

    # Simulated list of perceptions over time
    perceptions = [
        {"temperature": "comfortable", "light": "bright"},
        {"sound": "birdsong", "smell": "fresh_air"},
        {"temperature": "warm", "humidity": "high"}
    ]

    # Run the agent with the simulated perceptions
    agent.run(perceptions, cycle_delay=2)
